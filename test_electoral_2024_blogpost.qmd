---
title: "Une analyse critique du Test Electoral RTBF 2024"
format: html
---

```{r}
#| echo: false
#| output: false
library(tidyverse)
library(ComplexHeatmap)
library(viridis)
library(patchwork)
library(formattable)
library(plotly)
```

![](images/test_electoral_banner.png)

Publié le Mercredi 22 mai 2024 par Alexandre Irrthum. Update le 8 juin 2024.

Le [Test Électoral 2024](https://www.rtbf.be/test-electoral){target="_blank"} nous est vendu à grand renfort de publicité, sur les ondes de la RTBF et ailleurs, comme l'outil parfait pour l'électeur indécis qui veut se faire une opinion sur l'offre des partis politiques belges avant les élections de juin 2024.  Un tel test est un outil précieux dans une démocratie, mais il doit être transparent sur sa méthodologie et ouvert à un examen minutieux.

Pour rappel, le Test nous demande de répondre à 35 questions, compare nos réponses à celles des partis politiques et nous indique le (ou les) parti(s) dont nous sommes le plus proche. Un aspect important du test est le système de pondération, qui donne plus ou moins de poids à une réponse particulière dans le calcul des scores des partis, en fonction de la question et du parti considérés. Enfin, un système de boost permet de donner plus de poids aux questions qui nous semblent particulièrement importantes.

Je me suis penché sur le Test Électoral avec mon regard de professionnel de l'analyse des données et du développement des systèmes informatiques, même si mon domaine d'expertise est la recherche sur le cancer et pas les sciences politiques.

Dans la première partie de ce texte, je rappelle pourquoi la transparence est importante pour un outil comme le Test Électoral et pourquoi cette transparence fait ici défaut. Ensuite, je montre pourquoi le Test pose question sur plusieurs aspects méthodologiques. Enfin, dans la dernière partie, je fais une série de propositions qui pourraient améliorer le test au niveau transparence et méthodologie.

Certaines des analyses présentées ici ont été postées sur X/Twitter, sous une forme plus condensée.

### L'importance de la transparence

D'après le rédacteur en chef de *La Libre Belgique*, près d'un million de Belges auraient fait le Test Électoral le jour de son lancement national ([lien](https://x.com/ddemeeus/status/1779946520374022479){target="_blank"}). Dans une interview à *De Standaard*, deux des auteurs du Test mentionnent que le Test aurait atteint près de 3,5 millions de Flamands lors des élections précédentes ([lien](https://www.standaard.be/cnt/dmf20240414_97229245){target="_blank"}). On imagine l'impact que le Test peut avoir sur les résultats des élections. 

Seule la transparence permet à des personnes indépendantes de s'assurer que le Test est dépourvu autant que possible de biais méthodologiques. Par ailleurs, un Test qui n'apparaîtrait pas comme irréprochable risque d'accréditer l'idée regrettable des "élections, piège à cons" et de favoriser la désaffection des électeurs. Enfin, le manque de transparence du Test et le fait qu'il ne puisse pas être facilement analysé risquent aussi de le rendre plus facile à exploiter par ceux qui voudraient tenter de manipuler l'élection. Soyons clair, je ne pense pas du tout qu'il y ait une volonté malicieuse ou une ingérence dans le cas du Test Électoral. Mais la meilleure façon de prévenir les manipulations reste, à mon avis, la transparence totale, au niveau des données, du code informatique et des méthodes.

La méthode de calcul des scores des partis et la méthodologie utilisée n'étant pas disponibles (à part de vagues explications dans les FAQ), j'ai demandé au Test Électoral, via Twitter et par email, de me fournir ces informations. Malgré mes rappels, je n'ai pas reçu de réponse (voir l'annexe 2 pour la ligne du temps de mes tentatives de communication avec le Test Électoral). Update au 26 mai 2024: suite à un email aux deux responsables académiques du Test (Stefaan Walgrave, UAntwerp, Benoît Rihoux, UCLouvain) le 22 mai, Stefaan Walgrave m'a contacté et nous avons eu une conversation au sujet du Test le 24 mai.

N'ayant pas reçu de réponse à ma demande d'informations sur les méthodes, je me suis plongé dans le code informatique pour identifier et extraire la méthode de calcul et la matrice de pondération des réponses.

![](images/platform_code.png)

Pour être certain de la comprendre, j'ai répliqué exactement la méthode de calcul (y compris l'effet des réponses "Pas d'opinion" et l'effet du Boost) dans mon propre code informatique. Ces données et ce code, qui permettent d'étudier en détail le comportement du Test, sont disponibles [ici](https://github.com/AlexIrrthum/test_electoral_2024){target="_blank"}

Notons en passant que le test électoral Le Soir/SudInfo/RTL/ULB est encore moins transparent sur les méthodes utilisées. Un analyste indépendant, Thomas Delclite, s'est livré à un exercice de transparence similaire à celui-ci pour cet autre test et il présente ce travail [ici](https://x.com/TDelclite/status/1780556695137079796){target="_blank"}

### Un test pas fiable

Qu'est-ce-qu'on veut dire par un test pas fiable?

Imaginons qu'une électrice fasse le Test Électoral en répondant comme si elle était d'accord à 100% avec l'un des partis. À la fin du questionnaire elle voit que son score correspond à 100% au parti en question. Logique. Maintenant, elle change d'avis sur une seule question. Elle est toujours d'accord avec le parti pour 34 des 35 propositions mais, à sa grande surprise, le score du parti est tombé à 69% et un autre parti est passé 1er. Pour ajouter à son incompréhension, elle fait le compte des propositions pour lesquelles elle est d'accord avec cet autre parti (maintenant 1er) et elle n'en trouve que 23 sur 35.

Que penserait l'électrice du Test Électoral ? Probablement qu'il n'est pas fiable (au sens où on ne peut pas s'y fier pour se faire une opinion). Et bien, c'est exactement ce qu'on observe dans la réalité.

{{< video images/test_electoral_demo.mov >}}

Si vous voulez faire cette expérience vous-même sur la plateforme du Test Électoral, j'explique dans l'annexe 1 comment vous y prendre.

Ce comportement s'explique par le système de pondération des réponses, certaines questions ayant 10 fois plus de poids que d'autres. Le Boost, combiné avec le système de pondération aberrant, rend les résultats du test encore plus instables.

Notons que la question choisie ici est particulièrement problématique (j'explique plus loin pourquoi), mais l'instabilité existe aussi, dans une moindre mesure, avec beaucoup d'autres questions. Plusieurs personnes m'ont fait part de leur perplexité face au côté erratique du test.

![](images/confused_voter.webp){width=80%}

### Les partis de gauche, tous pareils ?

Ce qui m'a poussé au départ à essayer de comprendre le fonctionnement du Test Électoral 2024, c'est la difficulté du Test à différencier les partis de gauche. On voit ci-dessous les résultats d'un test réel, où les trois partis souvent considérés comme de gauche, Écolo, PS et PTB, reçoivent tous le même score. Les cinq questions supplémentaires, censées départager les partis avec des scores trop proches, ont brouillé les cartes encore plus.

![](images/test_results.png){width=80%}

Même si on modifie les réponses, les trois partis de gauche, et en particulier PS et Écolo, auront toujours des scores très proches. On comprend pourquoi le Test Électoral a difficile à départager les partis de gauche quand on compare le nombre de réponses différentes entre partis, comme dans cette table:

```{r}
#| echo: false
#| warning: false
parties <- c('PTB', 'PS', 'Ecolo', 'Défi', 'Les Engagés', 'MR')
qa <- read_csv('data/base_french_answers.csv')
qa <- qa %>%
        mutate(across(parties, ~ case_when(. == 'agree' ~ TRUE,
                                           . == 'disagree' ~ FALSE,
                                           . == 'undecided' ~ NA))) %>%
        relocate(parties)
# remove useless Q9
qa <- qa %>% na.omit()
questions <- qa %>% pull(question)
resumes <- qa %>% pull(short)
qa_for_heatmap <- qa %>%
        dplyr::select(-c(question, id, short)) %>%
        as.matrix()
qa_for_heatmap <- -1 * qa_for_heatmap
rownames(qa_for_heatmap) <- paste(questions, resumes)
dist2(t(qa_for_heatmap), pairwise_fun = function(x, y) sum(x != y))
```

PS et Ecolo ne diffèrent dans leurs réponses que pour 5 questions, avec des distances faibles pour le trio PS-Ecolo-PTB comparés aux autres partis. Donc ces partis sont plus difficilement différenciables que les autres.
 
Le risque, c'est que l'électeur de gauche indécis, pas aidé par le questionnaire, décide de voter au hasard, pour le (ou la) plus sympa, ou "utile". On comprend comment ceci peut pénaliser le parti qui aurait dû légitimement recevoir son suffrage sur base de préférences mieux définies.

On pourrait dire que ceci reflète une proximité réelle entre partis de gauche. Peut-être, mais c'est précisément pour cette raison qu'il est important de choisir des questions discriminantes. Le but du Test n'est pas de mettre en évidence les convergences entre partis, mais d'aider l'électeur à faire la différence entre ceux-ci.

### La matrice des poids

Au coeur de l'algorithme de calcul des scores des partis se trouve une matrice de pondération, qui attribue une valeur à chaque couple question x parti. Si vous êtes d'accord avec un parti pour une question particulière, son score augmente de la valeur correspondante (en absence de boost). Le graphique suivant est une représentation de cette matrice. Vous pouvez observer les valeurs exactes des poids en promenant votre pointeur de souris dans la matrice (c'est la valeur z). Le nombre à côté du nom de la question est la moyenne des poids d'une question pour les six partis.

```{r}
#| echo: false
#| warning: false
weights <- read_csv('data/base_french_weights.csv')
themes <- read_csv('data/base_french_themes.csv')
weights <- weights %>% left_join(themes)
weights <- weights %>%
        rowwise() %>%
        mutate(weight_mean=mean(c_across(all_of(parties))))
q <- weights %>% pull(question)
s <- weights %>% pull(short)
m <- weights %>% pull(weight_mean)
qsm <- paste(q, ' ', s, ' (', round(m, digits=3), ')', sep='')
weights$question <- factor(qsm, levels=rev(qsm))
weight_matrix <- as.matrix(weights %>% dplyr::select(all_of(parties)))
weight_matrix <- weight_matrix[nrow(weight_matrix):1,]
rownames(weight_matrix) <- rev(qsm)
fig <- plot_ly(x=colnames(weight_matrix), y=rownames(weight_matrix), z = weight_matrix, type = "heatmap")  %>%
    layout(yaxis = list(tickfont = list(size = 9)))
config(fig, displayModeBar = FALSE)
```

Les poids varient entre questions et, pour une question particulière, entre partis. On voit que les écarts entre questions sont généralement plus importants que les écarts entre partis.

On note aussi une valeur de poids beaucoup plus élevée pour Défi (17) pour la question "Police reconnaissance faciale" par rapport aux autres partis. Cette valeur pose problème parce que selon que l'électeur réponde "Pas d'accord" ou "D'accord" à cette question, Défi aura déjà un avantage (ou un désavantage selon la réponse) disproportionné au niveau de son score, ce qui fait qu'il va trop facilement (ou trop difficilement) se retrouver premier au final.

### Des sujets qui ne comptent pas

On voit dans l'extrait de la matrice des poids ci-dessous que les quatre questions immigration/intégration ont un poids (0.55) 10x moindre que la question "Place des cyclistes sur routes" (5.45), et donc un effet négligeable sur le résultat.

```{r, fig.height=2}
#| echo: false
#| warning: false
weight_matrix <- weight_matrix[c(36-35, 36-34, 36-30, 36-13, 36-2),] 
fig <- plot_ly(x=colnames(weight_matrix), y=rownames(weight_matrix), z = weight_matrix, type = "heatmap")  %>%
    layout(yaxis = list(tickfont = list(size = 12)))
config(fig, displayModeBar = FALSE)
```

Ceci est trompeur parce que l'électeur pour lequel ces questions sur l'immigration/intégration sont importantes (quelle que soit son opinion) aura l'impression que ses préoccupations sont largement prises en compte dans le Test, avec quatre questions, alors que ses réponses auront très peu d'impact sur le classement des partis. La seule réponse sur la place des cyclistes aura plus de deux fois plus d'impact que les quatre questions immigration/intégration réunies. Sur quelle base justifie-t-on cette forme de déclassement d'un sujet par rapport à un autre ?

Au delà des poids donnés aux différents sujets, le choix des sujets peut se discuter et chacun aura sa propre opinion, forcément subjective, sur les sujets importants. Mais en 2024, avec un monde qui brûle littéralement, l'effondrement de la biodiversité, des guerres à nos portes et une exacerbation des haines, on pourrait s'attendre à des questions sur la décroissance, la protection du vivant, la démocratie participative, la désobéissance civile, le contrôle des réseaux sociaux. Et pas une seule question sur la culture...

### L'impossible objectivité

Comme l'expliquent les FAQ du Test, les poids utilisés pour le calcul des scores sont obtenus par une analyse automatique des textes des programmes des partis, sur base de la fréquence de mots clés thématiques.

J'ai calculé les fréquences de mots clés pour trois thèmes, police/justice, immigration/intégration et laïcité/cultes (les listes de mots clés sont respectivement [ici](https://github.com/AlexIrrthum/test_electoral_2024/blob/master/data/keywords_police_justice.txt){target="_blank"}, [ici](https://github.com/AlexIrrthum/test_electoral_2024/blob/master/data/keywords_immigration_integration.txt){target="_blank"} et [ici](https://github.com/AlexIrrthum/test_electoral_2024/blob/master/data/keywords_laicite_cultes.txt){target="_blank"}). Je n'ai pas obtenu de pdf du programme pour le PTB, donc le PTB manque dans cette analyse. Le graphique suivant représente la fréquence des différents thèmes dans les pages des programmes des partis.

```{r}
#| echo: false
#| warning: false
#| out-width: 100%
counts <- read_csv("data/keyword_counts.csv")
counts <- counts %>%
    mutate(kw_ratio=100 * kw_count / word_count) %>%
    mutate(kw_ratio=if_else(is.na(kw_ratio), 0, kw_ratio)) %>%
    mutate(topic=as.factor(topic))
counts_summary <- counts %>%
    group_by(party, topic) %>%
    summarize(kw_sum=sum(kw_count), word_sum=sum(word_count)) %>%
    mutate(kw_ratio=100*kw_sum/word_sum) %>%
    mutate(party_label=paste('% global mots', topic, ' = ', round(kw_ratio, 3))) %>%
    mutate(y_position=case_when(topic=='immigration' ~ 10.5,
                                topic=='laicite/cultes' ~ 8.3,
                                topic=='police/justice' ~ 6.1))
counts %>%
    ggplot(aes(x=page, y=kw_ratio, fill=topic)) +
    geom_bar(stat='identity', position='stack') +
    facet_grid(party ~ .) +
    labs(x="Numéro de page du programme au format pdf", y="% de mots clés dans la page") +
    geom_text(data=counts_summary,
              mapping=aes(x=500,
                          y=y_position,
                          color=topic,
                          label=party_label),
              size=3,
              hjust=0) +
    scale_fill_manual(values=c("#FFB000", "#DC267F", "#648FFF")) +
    scale_color_manual(values=c("#FFB000", "#DC267F", "#648FFF"))
```

Dans le graphique, le panneau Défi montre les fréquences des mots clés calculées sur le programme en cinq parties de [Défi](https://www.defi.be/nos-publications/){target="_blank"}. On voit que Défi parle un peu moins du thème police/justice auquel se rattache la question "Police reconnaissance faciale" que les autres partis. On voit aussi que le faible poids des questions immigration/intégration est bien lié à une place faible de ce thème dans les programmes.

Lorsque je l'ai alerté sur la valeur apparemment anormale de Défi pour la question "Police reconnaissance faciale", Stefaan Walgrave m'a transmis le programme communiqué par Défi aux auteurs du Test. Le second panneau Défi_plus montre les fréquences calculées pour ce document, avec une fréquence beaucoup plus élevée pour le thème police/justice. L'explication de cette différence, c'est que ce document n'est pas le programme sensu stricto, mais le programme plus les notes thématiques, dont la longue et dense note "Justice - 100 propositions".

On voit comment les choix des partis sur ce qu'ils communiquent exactement aux auteurs du Test peuvent avoir un effet considérable sur les poids et les scores calculés. Chaque parti vient avec ses marottes du moment liées à la personnalité du chef (ou de l'équipe dirigeante) et le style de language même du document programme (language imagé avec beaucoup d'exemples, language plus académique, language concis ou verbeux) aura une influence un peu arbitraire sur les poids et les scores calculés.

Les listes de mots clés elle-mêmes ne sont jamais totalement objectives. Dans mes mots clés immigration/intégration, je n'ai pas inclus de mots liés à la religion, considérant que ces mots se rattachent davantage au thème laïcité/cultes. Et dans mon cas, pour simplifier l'analyse, je me suis aussi limité à des mots clés simples. Mais d'autres choix sont défendables, avec un impact sur les scores calculés.

Enfin, le rattachement d'une question à un thème ou un autre est aussi parfois subjectif et va modifier son poids. La question sur la place des cyclistes sur les routes est-elle purement une question transports/mobilité, ou est-elle aussi une question sur l'environnement ?

La vérité c'est qu'il n'y a jamais de méthodologie purement objective et que chaque décision méthodologique a une influence plus ou moins grande sur les scores des partis. Une façon pragmatique de s'en sortir en évitant des débats incessants est de limiter au maximum (voire de supprimer) l'influence d'un système de pondération forcément discutable.

```{r}
#| echo: false
#| warning: false
weights <- read_csv('data/base_french_weights.csv')
themes <- read_csv('data/base_french_themes.csv')
weights <- weights %>% left_join(themes)
weights <- weights %>%
        rowwise() %>%
        mutate(weight_mean=mean(c_across(all_of(parties))))
q <- weights %>% pull(question)
s <- weights %>% pull(short)
m <- weights %>% pull(weight_mean)
qsm <- paste(q, ' ', s, ' (', round(m, digits=3), ')', sep='')
weights$question <- factor(qsm, levels=rev(qsm))
weights_french_long <- weights %>%
        dplyr::select(question, parties, theme, theme_color) %>%
        pivot_longer(cols=-c(question, theme, theme_color), names_to='parti', values_to='weight') %>%
        mutate(parti=factor(parti, levels=parties)) 

parties <- c('Vlaams Belang','Vooruit','Open VLD','CD&V','Groen','PVDA','NVA')
weights <- read_csv('data/base_flemish_weights.csv')
weights <- weights %>%
        rowwise() %>%
        mutate(weight_mean=mean(c_across(all_of(parties))))
q <- weights %>% pull(question)
s <- weights %>% pull(short)
m <- weights %>% pull(weight_mean)
qsm <- paste(q, ' ', s, ' (', round(m, digits=3), ')', sep='')
weights$question <- factor(qsm, levels=rev(qsm))
weights_flemish_long <- weights %>%
        dplyr::select(-id, -short, -weight_mean) %>%
        pivot_longer(cols=-question, names_to='parti', values_to='weight') %>%
        mutate(parti=factor(parti, levels=parties)) 
# reset parties to french-speaking parties
parties <- c('PTB', 'PS', 'Ecolo', 'Défi', 'Les Engagés', 'MR')

p1 <- weights_french_long %>%
        ggplot(aes(x=weight, fill=after_stat(x))) +
        geom_histogram(binwidth=1, center=0.5) +
        scale_fill_viridis(discrete=FALSE) +
        annotate('text', x=12.5, y=10, label='Défi x Rec. faciale') +
        annotate('segment', x=15, xend=17.5, y=8, yend=2, color='red') +
        labs(title='Poids pour 35 questions de base FR', x='Poids', y="Nombre de valeurs dans l'intervalle") +
        theme(legend.position='none')
p2 <- weights_flemish_long %>%
        ggplot(aes(x=weight, fill=after_stat(x))) +
        geom_histogram(binwidth=1, center=0.5) +
        scale_fill_viridis(discrete=FALSE) +
        annotate('text', x=7, y=32, label='Groen x Abortus 18 weken') +
        annotate('segment', x=8, xend=10.5, y=30, yend=2, color='red') +
        labs(title='Poids pour 35 questions de base FL', x='Poids', y="Nombre de valeurs dans l'intervalle") +
        theme(legend.position='none') +
        scale_x_continuous(breaks=c(0, 5, 10))
# We don't show the histograms as they don't add a lot to the story
#p1 + p2
```

### Le sens des mots

Un autre aspect déconcertant du Test est la façon dont les textes des programmes des partis sont parfois traduits en D'accord / Pas d'accord. On voit ici comment les textes des partis ont été interprétés pour la question "L'UE ne doit pas prendre part au conflit en Palestine". La question est évidemment ambigüe, mais, sur base des textes présentés, les partis semblent d'accord sur une intervention diplomatique, mais pas militaire. 

![](images/question_20.png){width=80%}

Difficile alors de comprendre pourquoi certains se retrouvent avec "D'accord" et d'autres avec "Pas d'accord". Les FAQ expliquent que les partis ont validé toutes les réponses. Ça laisse songeur.

### Une précieuse base de données

Juste avant que le Test Électoral ne nous présente les scores des partis, l'ensemble de nos réponses est envoyé, de manière anonyme, vers un serveur et sauvegardé dans une base de données. Les auteurs mentionnent ceci sur la page [Vie privée](https://www.rtbf.be/test-electoral#/privacy).

Quand on regarde ce qui est envoyé exactement au serveur, on voit les réponses aux questions et aussi le temps mis pour répondre à chaque question (en millisecondes). C'est une information intéressante parce qu'elle indique si l'électeur est hésitant sur certaines questions.

![](images/request_payload.png){width=80%}

Il n'est pas clairement dit si d'autres informations sont sauvegardées avec les réponses aux questions, comme par exemple l'adresse IP de l'utilisateur. Du point de vue de l'exploitation de la base de données des réponses, il serait logique de sauvegarder l'adresse IP parce qu'elle permet, dans certains cas, d'obtenir une meilleure estimation du nombre d'utilisateurs uniques et de ne garder qu'un jeu de réponses par utilisateur.

Cette base de données, qui recense les préférences et les hésitations de millions d'électeurs, représente une ressource inestimable pour les chercheurs en sciences politiques, pour les partis politiques mais aussi pour quiconque voudrait s'ingérer dans la vie politique du pays. Un parti politique muni de cette base de données (ou de celle du Test Électoral 2019) pourrait modifier son programme, sa campagne, ou même ses réponses "D'accord/Pas d'accord" au Test pour améliorer son score et grappiller des suffrages. Un agitateur pourrait l'utiliser pour identifier les plans de clivage au sein de notre société ou au sein d'un groupe de citoyens et les exploiter.

D'où la question la plus importante liée au Test Électoral: qui exactement a accès ou aura accès à la base de données des réponses, et qui a eu accès à celle de l'édition précédente du Test (2019). Le fichier de la base de données, avec les réponses de millions d'électeurs, tient sur une modeste clé USB et, sauf mise en place d'un niveau de sécurité informatique quasi militaire, il est difficile de se prémunir contre sa fuite. Plutôt que de miser sur la "sécurité par l'obscurité" et de risquer une asymétrie d'information préjudiciable à la démocratie (par exemple si un seul parti politique avait accès au fichier), la meilleure solution me semble être ici aussi la transparence, avec la mise dans le domaine public de ces données.

### Et après, on fait quoi ?

Voici ma liste de propositions pour un futur Test Électoral:

- partager les données (listes de mots clés, matching questions-thèmes, matrice des poids...) et le code informatique (calcul des poids, calcul des scores des partis...) qui soutiennent le Test.
- préciser quelle a été la contribution exacte de chaque collaborateur.
- divulger d'éventuels conflits d'intérêts entre les contributaires au Test et les partis politiques.
- simplifier, voire abandonner le système de pondération actuel. De manière générale, appliquer le principe KISS (Keep It Short and Simple) à la méthodologie.
- si on conserve le système de pondération, garder la variation des poids dans des limites raisonnables.
- s'assurer que les nombres de questions discriminantes entre paires de partis ne soient pas trop disproportionnés.
- ne pas hésiter à mettre en cause les réponses "D'accord/Pas d'accord" des partis quand elles sont en contradiction avec les textes des programmes.
- utiliser et diffuser des visualisations de données comme je l'ai fait ici pour mettre en évidence des anomalies (ou absence d'anomalies) dans le test.
- prévoir une courte phase de beta-testing avec des testeurs indépendants des auteurs du Test avant sa mise en production.
- divulger la liste complète des personnes ou organisations qui ont eu ou auront accès à la base de données des réponses (y compris pour les éditions précédentes).
- envisager de rendre publique la base de données des réponses au Test.

Lors des élections futures, les tests électoraux les plus en vue seront probablement des robots conversationnels type ChatGPT. Ceci permet d'envisager un test électoral vraiment pertinent, qui s'adapte aux priorités de l'électeur. Mais, faute d'une transparence irréprochable et de tests approfondis, ceci décuple les risques de biais et les possibilités de manipulation. Il est important de se préparer à cette évolution.

### Remerciements

Un grand merci à Shean Massey pour son aide avec le déchiffrage du code javascript et à plusieurs collègues pour leurs commentaires.

### Qui je suis ?

Alexandre Irrthum. J'ai une formation de bioingénieur et un doctorat en sciences biomédicales (génétique). Mon travail est le conseil scientifique et l'analyse de données cliniques et moléculaires dans le domaine de la recherche sur le cancer du sein, et la mise en place de systèmes informatiques pour l'acquisition et la gestion de ces données.

### Annexe 1: une expérience pour comprendre l'instabilité "pathologique" du test

Sur la plateforme du Test, on répond exactement avec cette séquence de 35 réponses 'DPPDPPDPDPPDPDDDDDPDPDDDDDPDPPPPPDP' (avec D = D’accord et P = Pas d’accord). On choisit ensuite "Continuer sans coup de boost". On voit que Défi arrive 1er avec 100%. Ensuite on clique sur "Modifier vos réponses" et on modifie la réponse à la question 6 “Police reconnaissance faciale” en D’accord + Boost. On voit que Les Engagés sont maintenant 1er, avec seulement 23/35 réponses qui matchent alors qu'on est 34/35 qui matchent pour Défi. 

### Annexe 2: ligne du temps des communications avec le Test Électoral

Voici la ligne du temps de mes tentatives de communication avec le Test Électoral, via X/Twitter et par email. @TestElectoral sur X/Twitter et l'adresse email info@testelectoral.be sont les deux canaux recommandés pour contacter le Test Électoral.

![](images/twitter_email_timeline.png)

### Annexe 3: Analyse du Test par algorithme aléatoire

Un test intéressant est de regarder quel parti arrive premier le plus souvent si on répond au hasard aux questions. En répetant la création d'un jeu de 35 réponses aléatoires 100000x et en regardant quel parti arrive premier, on voit que Défi et MR arrivent premier le + souvent, et le PS arrive premier le - souvent. 

![](images/random_winners.png){width=80%}

Bien sûr, les élécteurs ne répondent pas de façon aléatoire, mais cette analyse dit cependant quelque chose d'intéressant sur l'algorithme de calcul des scores des partis et ses biais potentiels. Deux effets expliquent probablement, au moins en partie, la répartition observée. Le premier est l'effet de la valeur aberrante de 17.5 pour la question "Police reconnaissance faciale" pour Défi. Il y a une chance sur deux de choisir la réponse "Pas d'accord" et dans ce cas Défi reçoit déjà 17.5%, ce qui le rapproche fortement de la première place. Le deuxième effet probable qui explique pourquoi MR est souvent premier et les partis de gauche plus rarement est la proximité des partis de gauche dans le questionnaire qui fait que leurs scores seront souvent très proches les uns des autres. Même si ces effets sont montrés ici avec un algorithme aléatoire, ils existent aussi pour des réponses "réelles".